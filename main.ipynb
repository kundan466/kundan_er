{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf7103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint, time\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')\n",
    "import matplotlib.pyplot as mtp  \n",
    "from sklearn import svm,naive_bayes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as mtp  \n",
    "from sklearn import svm,naive_bayes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer, sent_tokenize\n",
    "from attributes import domain_dict\n",
    "\n",
    "import ast #for updating in attributes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38edc2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install language-tool-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed8b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing for testing to remove spelling mistakes\n",
    "\n",
    "# importing the library  \n",
    "import language_tool_python  \n",
    "  \n",
    "# creating the tool  \n",
    "spell_correction = language_tool_python.LanguageTool('en-US')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2908cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0424cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f187eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install graphviz Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b95e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"modifications_dataset.csv\")\n",
    "df = pd.read_csv(\"learning_dataset.csv\")\n",
    "\n",
    "\n",
    "# Drop any rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "X = df[['Word', 'POS']].apply(lambda x: ' '.join(x), axis=1)\n",
    "y = df['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# Convert the words to a matrix of token counts\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "# Convert the test words to a matrix of token counts\n",
    "X_test_counts = vectorizer.transform(X_test.apply(lambda x: ' '.join(x)))\n",
    "# Train a Naive Bayes classifier\n",
    "clf = MultinomialNB().fit(X_train_counts, y_train)\n",
    "# In[83]:\n",
    "# Use the classifier to predict the labels of the test set\n",
    "y_pred = clf.predict(X_test_counts)\n",
    "# # Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#geeksforgeeks\n",
    "\n",
    "text=input(\"Enter text:\")\n",
    "text = text.lower()\n",
    "bad_chars = [';', ':', '!', \"*\",\"@\",\"(\",\")\",\",\",\"be \",\" be\"]\n",
    "# using replace() to\n",
    "# remove bad_chars\n",
    "for i in bad_chars:\n",
    "\ttext = text.replace(i, '')\n",
    "# print(text)\n",
    "sentences = sent_tokenize(text)\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "array_m = []\n",
    "for sentence in sentences:\n",
    "    # Tokenize the sentence\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # POS tagging using the Averaged Perceptron Tagger\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Lemmatize each word based on its POS tag\n",
    "    lemmas = []\n",
    "    for word, tag in pos_tags:\n",
    "        if tag in ['NN','NNS', 'NNPS', 'POS']:\n",
    "            lemma = lemmatizer.lemmatize(word, pos='n')\n",
    "        elif tag in ['RB', 'VBD', 'VBN', 'VBG', 'VB']:\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        else:\n",
    "            lemma = word\n",
    "        lemmas.append((lemma, tag))\n",
    "    \n",
    "    array_m.append(lemmas)\n",
    "\n",
    "# print(array_m)\n",
    "df_m = pd.DataFrame(array_m)\n",
    "array = []\n",
    "for i in range(df_m.shape[0]):\n",
    "    for j in range(df_m.shape[1]):\n",
    "        word_pos = df_m.iloc[i, j] # Updated indexing with iloc\n",
    "        if word_pos is not None:\n",
    "            word_counts = vectorizer.transform([' '.join(word_pos)])\n",
    "            label = clf.predict(word_counts)[0]\n",
    "            if label != \"O\":\n",
    "                temp = df_m.iloc[i, j] + (label,) # Updated indexing with iloc\n",
    "                array.append(temp)\n",
    "array_list = [tuple(item) for item in array]\n",
    "# print(array_list)\n",
    "# code if text has many,more,various make next NN as NNS\n",
    "\n",
    "output = array_list\n",
    "flag = False\n",
    "for i, (word, tag, label) in enumerate(output):\n",
    "    if word in ['many','more','various','numerous','abundant','multiple','several','plenty']:\n",
    "        flag = True\n",
    "        output[i] = (word, 'JJ', 'O')\n",
    "    if flag and label == 'Entity':\n",
    "        output[i] = (word, 'NNS', 'Entity')\n",
    "        flag = False\n",
    "\n",
    "# if 'be' is followed by a relation then delete it\n",
    "for i in range(len(output) - 1):\n",
    "    if output[i][0] in ['be','being','been'] and output[i+1][2] == 'Relation':\n",
    "        output[i] = (output[i][0], 'VB', 'O')\n",
    "# print(output)\n",
    "array_list = output\n",
    "#code starting to update attribute\n",
    "\n",
    "# to read the existing domain_dict from attributes.py\n",
    "with open(\"attributes.py\", \"r\") as f:\n",
    "    attributes_contents = f.read()\n",
    "\n",
    "# Convert the domain_dict string to a dictionary\n",
    "attributes_dict = ast.literal_eval(attributes_contents.split(\"=\")[1].strip())\n",
    "def synonym_antonym_extractor(phrase):\n",
    "    synonyms = []\n",
    "    # Extract synonyms for the given phrase\n",
    "    for syn in wn.synsets(phrase):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "    synonyms = set(synonyms)\n",
    "    # Find common keys (entity names) between synonyms and domain_dict\n",
    "    common_keys = synonyms.intersection(domain_dict.keys())\n",
    "    return common_keys\n",
    "\n",
    "# Initialize variables\n",
    "entity_ids = {}\n",
    "entities = []\n",
    "relations = []\n",
    "entity_counter = 1\n",
    "relation_counter = 1\n",
    "new_attributes_dict = {}\n",
    "\n",
    "# Create the entity_ids dictionary\n",
    "for i, (word, pos, label) in enumerate(array_list):\n",
    "    if label == 'Entity' or label == 'Relation':\n",
    "        entity_ids.setdefault(word, f\"ent_{len(entity_ids) + 1}\")\n",
    "\n",
    "for i, (word, pos, label) in enumerate(array_list):\n",
    "    if label == 'Entity':\n",
    "        # Create entity dictionary\n",
    "        entity_dict = {\n",
    "            \"id\": entity_ids[word],\n",
    "            \"name\": word,\n",
    "            \"POS\": pos,\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"attribute\": []\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        entities.append(entity_dict)\n",
    "    elif label == 'Relation':\n",
    "        relation_name = word\n",
    "        if len(entities) >= 1:\n",
    "            relation_entity_ids = []\n",
    "            relation_entity_pos = []\n",
    "            # Find the entity names preceding and succeeding the relation\n",
    "            for j in range(i-1, -1, -1):\n",
    "                if array_list[j][2] == 'Entity':\n",
    "                    entity_name = array_list[j][0]\n",
    "                    if entity_name in entity_ids:\n",
    "                        relation_entity_ids.append(entity_ids[entity_name])\n",
    "                        relation_entity_pos.append(array_list[j][1])\n",
    "                        break\n",
    "            for j in range(i+1, len(array_list)):\n",
    "                if array_list[j][2] == 'Entity':\n",
    "                    entity_name = array_list[j][0]\n",
    "                    if entity_name in entity_ids:\n",
    "                        relation_entity_ids.append(entity_ids[entity_name])\n",
    "                        relation_entity_pos.append(array_list[j][1])\n",
    "                        break\n",
    "            # Create relation dictionary\n",
    "            if len(relation_entity_ids) == 2:\n",
    "                relations.append({\n",
    "                    \"id\": f\"r{relation_counter}\",\n",
    "                    \"name\": relation_name,\n",
    "                    \"entity_ids\": [\n",
    "                        {\n",
    "                            \"id\": relation_entity_ids[0],\n",
    "                            \"pos\": relation_entity_pos[0]\n",
    "                        },\n",
    "                        {\n",
    "                            \"id\": relation_entity_ids[1],\n",
    "                            \"pos\": relation_entity_pos[1]\n",
    "                        }\n",
    "                    ]\n",
    "                })\n",
    "                relation_counter += 1\n",
    "\n",
    "# Create the ER diagram dictionary\n",
    "er_diagram = {\n",
    "    \"entities\": entities,\n",
    "    \"relations\": relations\n",
    "}\n",
    "\n",
    "# Convert the ER diagram dictionary to JSON string\n",
    "json_ = json.dumps(er_diagram, indent=4)\n",
    "# print(json_)\n",
    "# Remove entity duplicates\n",
    "entities_dict = {}  # Dictionary to store unique entities\n",
    "\n",
    "json_data = json.loads(json_)  # Convert JSON string to dictionary\n",
    "\n",
    "# Iterate over entities\n",
    "for entity in json_data['entities']:\n",
    "    entity_id = entity['id']\n",
    "    if entity_id in entities_dict:\n",
    "        # Entity already exists, update its properties\n",
    "        existing_entity = entities_dict[entity_id]\n",
    "        existing_properties = existing_entity['properties']\n",
    "        new_properties = entity['properties']\n",
    "        for new_prop in new_properties:\n",
    "            new_attr = new_prop['attribute']\n",
    "            if new_attr not in [prop['attribute'] for prop in existing_properties]:\n",
    "                existing_properties.append(new_prop)\n",
    "    else:\n",
    "        # Add entity to dictionary\n",
    "        entities_dict[entity_id] = entity\n",
    "\n",
    "stored_relations = {}  # Dictionary to store unique relations\n",
    "i = 0\n",
    "\n",
    "while i < len(relations):\n",
    "    relation = relations[i]\n",
    "    duplicate_found = False\n",
    "    highest_degree_pos = {}\n",
    "\n",
    "    if relation['name'] in stored_relations:\n",
    "        duplicate_relation = stored_relations[relation['name']]\n",
    "        if set([entity['id'] for entity in duplicate_relation['entity_ids']]) == set(\n",
    "                [entity['id'] for entity in relation['entity_ids']]):\n",
    "            duplicate_found = True\n",
    "    else:\n",
    "        stored_relations[relation['name']] = relation\n",
    "\n",
    "    if not duplicate_found:\n",
    "        for other_relation in relations:\n",
    "            if other_relation != relation and other_relation['name'] == relation['name']:\n",
    "                if set([entity['id'] for entity in other_relation['entity_ids']]) == set(\n",
    "                        [entity['id'] for entity in relation['entity_ids']]) or \\\n",
    "                   set([entity['id'] for entity in other_relation['entity_ids']]) == set(\n",
    "                        [entity['id'] for entity in relation['entity_ids'][::-1]]):\n",
    "                    for entity in relation['entity_ids']:\n",
    "                        entity_id = entity['id']\n",
    "                        pos = entity['pos']\n",
    "                        if entity_id in highest_degree_pos:\n",
    "                            if pos == 'NNS' and highest_degree_pos[entity_id] == 'NN':\n",
    "                                highest_degree_pos[entity_id] = 'NNS'\n",
    "                        else:\n",
    "                            highest_degree_pos[entity_id] = pos\n",
    "\n",
    "                        other_entity = next(\n",
    "                            (other_entity for other_entity in other_relation['entity_ids'] if\n",
    "                             other_entity['id'] == entity_id), None)\n",
    "                        if other_entity:\n",
    "                            other_pos = other_entity['pos']\n",
    "                            if other_pos != pos:\n",
    "                                if other_pos == 'NNS' and pos == 'NN':\n",
    "                                    highest_degree_pos[entity_id] = 'NNS'\n",
    "\n",
    "                    duplicate_found = True\n",
    "                    break\n",
    "\n",
    "        for entity in relation['entity_ids']:\n",
    "            entity_id = entity['id']\n",
    "            pos = entity['pos']\n",
    "            highest_degree = highest_degree_pos.get(entity_id)\n",
    "            if highest_degree and highest_degree != pos:\n",
    "                entity['pos'] = highest_degree\n",
    "\n",
    "        i += 1\n",
    "    else:\n",
    "        relations.pop(i)\n",
    "\n",
    "seen_relations = set()  # Set to keep track of seen relations\n",
    "i = 0\n",
    "while i < len(relations):\n",
    "    relation = relations[i]\n",
    "    # Check if the relation has been seen before\n",
    "    if (relation['name'], tuple(sorted(e['id'] for e in relation['entity_ids']))) in seen_relations:\n",
    "        relations.pop(i)\n",
    "    else:\n",
    "        seen_relations.add((relation['name'], tuple(sorted(e['id'] for e in relation['entity_ids']))))\n",
    "        i += 1\n",
    "\n",
    "# Convert dictionary back to list of entities\n",
    "entities_list = list(entities_dict.values())\n",
    "\n",
    "# Create the ER diagram dictionary\n",
    "er_diagram = {\n",
    "    \"Entities\": entities_list,\n",
    "    \"Relationships\": relations\n",
    "}\n",
    "\n",
    "# Convert list of entities back to JSON string\n",
    "output_json = json.dumps(er_diagram, indent=4)\n",
    "\n",
    "# print(output_json)\n",
    "entities_final = []  # List to store final entities\n",
    "attributes = {}  # Dictionary to store attributes\n",
    "\n",
    "# Extract attributes from existing entities\n",
    "for entity in er_diagram[\"Entities\"]:\n",
    "    word = entity[\"name\"]\n",
    "    entity_attributes = [attr[\"attribute\"] for attr in entity[\"properties\"]]\n",
    "    attributes[word] = entity_attributes\n",
    "\n",
    "# Process entities\n",
    "for word, entity_attributes in attributes.items():\n",
    "    if word in domain_dict.keys():\n",
    "        # Entity attributes found in the domain_dict\n",
    "        entity_final = {\n",
    "            \"id\": entity_ids[word],\n",
    "            \"name\": word,\n",
    "            \"POS\": pos,\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"attribute\": attribute_name\n",
    "                } for attribute_name in dict.fromkeys(domain_dict[word])\n",
    "            ]\n",
    "        }\n",
    "    elif len(synonym_antonym_extractor(phrase=word)) != 0:\n",
    "        # Matching synonyms found in the domain_dict\n",
    "        matched_word = synonym_antonym_extractor(phrase=word)\n",
    "        for value in matched_word:\n",
    "            matched_attr = domain_dict[value]\n",
    "            user_input = input(\"We found these similar attributes: \" + \", \".join(matched_attr) +\n",
    "                               \" for Entity Name: \" + word + \". Do you want to add this (Yes/No):\")\n",
    "            new_attribute = matched_attr\n",
    "            if user_input in [\"yes\", \"Yes\"]:\n",
    "                # Add matched attributes to entity\n",
    "                entity_final = {\n",
    "                    \"id\": entity_ids[word],\n",
    "                    \"name\": word,\n",
    "                    \"POS\": pos,\n",
    "                    \"properties\": [\n",
    "                        {\n",
    "                            \"attribute\": attribute_name\n",
    "                        } for attribute_name in dict.fromkeys(domain_dict[value])\n",
    "                    ]\n",
    "                }\n",
    "                # Update domain_dict with new attributes\n",
    "                if new_attribute:\n",
    "                    new_attributes_dict[word] = set(new_attribute)\n",
    "                    attributes_dict.update(new_attributes_dict)\n",
    "                    # Convert the updated domain_dict back to a string\n",
    "                    new_attributes_contents = f\"domain_dict = {attributes_dict}\"\n",
    "                    # Write the updated domain_dict to attributes.py\n",
    "                    with open(\"attributes.py\", \"w\") as f:\n",
    "                        f.write(new_attributes_contents)\n",
    "                break\n",
    "            else:\n",
    "                input_string = input(\"Unfortunately we didn't find attributes for Entity Name: \" + word +\n",
    "                                     \", Please Enter attributes separated with space:\")\n",
    "                new_attribute = input_string.split()\n",
    "                # Add user-defined attributes to entity\n",
    "                entity_final = {\n",
    "                    \"id\": entity_ids[word],\n",
    "                    \"name\": word,\n",
    "                    \"POS\": pos,\n",
    "                    \"properties\": [\n",
    "                        {\n",
    "                            \"attribute\": attribute\n",
    "                        } for attribute in new_attribute\n",
    "                    ]\n",
    "                }\n",
    "                # Update domain_dict with new attributes\n",
    "                if new_attribute:\n",
    "                    new_attributes_dict[word] = set(new_attribute)\n",
    "                    attributes_dict.update(new_attributes_dict)\n",
    "                    # Convert the updated domain_dict back to a string\n",
    "                    new_attributes_contents = f\"domain_dict = {attributes_dict}\"\n",
    "                    # Write the updated domain_dict to attributes.py\n",
    "                    with open(\"attributes.py\", \"w\") as f:\n",
    "                        f.write(new_attributes_contents)\n",
    "                break\n",
    "    else:\n",
    "        # No attributes found, prompt user for input\n",
    "        input_string = input(\"Unfortunately we didn't find attributes for Entity Name: \" + word +\n",
    "                             \", Please Enter attributes separated with space:\")\n",
    "        new_attribute = input_string.split()\n",
    "        # Add user-defined attributes to entity\n",
    "        entity_final = {\n",
    "            \"id\": entity_ids[word],\n",
    "            \"name\": word,\n",
    "            \"POS\": pos,\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"attribute\": attribute\n",
    "                } for attribute in new_attribute\n",
    "            ]\n",
    "        }\n",
    "        # Update domain_dict with new attributes\n",
    "        if new_attribute:\n",
    "            new_attributes_dict[word] = set(new_attribute)\n",
    "            attributes_dict.update(new_attributes_dict)\n",
    "            # Convert the updated domain_dict back to a string\n",
    "            new_attributes_contents = f\"domain_dict = {attributes_dict}\"\n",
    "            # Write the updated domain_dict to attributes.py\n",
    "            with open(\"attributes.py\", \"w\") as f:\n",
    "                f.write(new_attributes_contents)\n",
    "    entities_final.append(entity_final)\n",
    "\n",
    "er_diagram = {\n",
    "    \"entities\": entities_final,\n",
    "    \"relations\": relations\n",
    "}\n",
    "\n",
    "# Print the ER diagram as JSON\n",
    "json_final = json.dumps(er_diagram, indent=4)\n",
    "print(json_final)\n",
    "# alligned graph\n",
    "\n",
    "# Load the ER diagram JSON data\n",
    "er_diagram = json.loads(json_final)\n",
    "\n",
    "# Create a Graphviz graph with left-to-right direction\n",
    "graph = Digraph(graph_attr={'rankdir': 'LR'})\n",
    "\n",
    "# Add the entities and their properties as nodes to the graph\n",
    "for entity in er_diagram['entities']:\n",
    "    entity_id = entity['id']\n",
    "    graph.node(entity_id, entity['name'], shape='rectangle', style='filled', fillcolor='lightblue')\n",
    "\n",
    "    for property in entity['properties']:\n",
    "        node_id = f\"{entity_id}_{property['attribute']}\"\n",
    "        graph.node(node_id, property['attribute'], shape='oval')\n",
    "        graph.edge(entity_id, node_id, dir='none')\n",
    "\n",
    "# Add the relations and their entities as nodes to the graph\n",
    "for relation in er_diagram['relations']:\n",
    "    relation_id = relation['id']\n",
    "    graph.node(relation_id, relation['name'], shape='diamond', style='filled', fillcolor='lightgrey')\n",
    "\n",
    "    entity_ids = relation['entity_ids']\n",
    "    from_id, to_id = entity_ids[0]['id'], entity_ids[1]['id']\n",
    "    l1 = '1' if entity_ids[0]['pos'] == 'NN' else 'm'\n",
    "    l2 = '1' if entity_ids[1]['pos'] == 'NN' else 'm'\n",
    "#     l2 = '1' if entity_ids[0]['pos'] in ['NNS','NNP'] else 'm'\n",
    "    graph.edge(from_id, relation_id, dir='none',label = l1)\n",
    "    graph.edge(relation_id, to_id, dir='none',label = l2)\n",
    "    \n",
    "# Render the Graphviz graph and save it as a PNG image\n",
    "graph.render('er_diagram', format='png')\n",
    "\n",
    "# Open the image using PIL library\n",
    "im = Image.open('er_diagram.png')\n",
    "\n",
    "# Display the image in the console\n",
    "im.show()\n",
    "# Function to update the dataset with a new word\n",
    "def update_dataset(word, pos, label, df):\n",
    "    # Check if the word and pos already exist in the dataset\n",
    "    if df[(df['Word'] == word) & (df['POS'] == pos)].empty:\n",
    "        # If not, create a new dataframe with the new row\n",
    "        new_row = pd.DataFrame({'Word': [word], 'POS': [pos], 'label': [label]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "    else:\n",
    "        # If the word and pos already exist in the dataset, update the label\n",
    "        df.loc[(df['Word'] == word) & (df['POS'] == pos), 'label'] = label\n",
    "    # Write the updated dataframe to the 'learning_dataset.csv' file\n",
    "    df.to_csv('learning_dataset.csv', index=False)\n",
    "    # Return the updated dataframe\n",
    "    return df\n",
    "#interactive learning\n",
    "\n",
    "# Iterate over the list of tuples\n",
    "feedback = input(\"How is the ERD(good/bad): \")\n",
    "\n",
    "if feedback == \"good\":\n",
    "    for word, pos, label in array_list:\n",
    "        # Call the update_dataset function for every word\n",
    "#         print(word,pos,label)\n",
    "        df = pd.read_csv(\"learning_dataset.csv\")\n",
    "        update_dataset(word, pos, label, df)\n",
    "    print(\"I sincerely appreciate your feedback and will use it to enhance our performance. Thank you!\")\n",
    "elif feedback == \"bad\":\n",
    "    for word, pos, label in array_list:\n",
    "        # Call the update_dataset function for every word\n",
    "#         print(word, pos, label)\n",
    "        df = pd.read_csv(\"learning_dataset.csv\")\n",
    "        update_dataset(word, pos, label, df)\n",
    "    # Take multiple words, their POS, and updated labels as input from the user\n",
    "    words = input(\"Enter words separated by space: \").split(' ')\n",
    "    poss = input(\"Enter POS tags separated by space: \").split(' ')\n",
    "    labels = input(\"Enter labels separated by space: \").split(' ')\n",
    "    for i in range(len(words)):\n",
    "        df = pd.read_csv(\"learning_dataset.csv\")\n",
    "        if ((df['Word'] == words[i]) & (df['POS'] == poss[i])).any():\n",
    "            update_dataset(words[i], poss[i], labels[i], df)\n",
    "        else:\n",
    "            print(\"I'm sorry, but the words you mentioned do not appear in the sentence. Could you please double-check?\")\n",
    "    print(\"Thank you for your valuable feedback. It helps me improve.\")\n",
    "else:\n",
    "    print(\"Please provide your feedback for me to improve.\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b4758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
